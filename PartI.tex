\documentclass[]{book}

%These tell TeX which packages to use.
\usepackage{array,epsfig}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsxtra}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{color}
\usepackage[utf8]{inputenc}

%Here I define some theorem styles and shortcut commands for symbols I use often
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem*{rmk}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem*{joke}{Joke}
\newtheorem{ex}{Example}
\newtheorem*{soln}{Solution}
\newtheorem{prop}{Proposition}

\newcommand{\lra}{\longrightarrow}
\newcommand{\ra}{\rightarrow}
\newcommand{\surj}{\twoheadrightarrow}
\newcommand{\graph}{\mathrm{graph}}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\Z}{\bb{Z}}
\newcommand{\Q}{\bb{Q}}
\newcommand{\R}{\bb{R}}
\newcommand{\C}{\bb{C}}
\newcommand{\N}{\bb{N}}
\newcommand{\M}{\mathbf{M}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\MM}{\mathscr{M}}
\newcommand{\HH}{\mathscr{H}}
\newcommand{\Om}{\Omega}
\newcommand{\Ho}{\in\HH(\Om)}
\newcommand{\bd}{\partial}
\newcommand{\del}{\partial}
\newcommand{\bardel}{\overline\partial}
\newcommand{\textdf}[1]{\textbf{\textsf{#1}}\index{#1}}
\newcommand{\img}{\mathrm{img}}
\newcommand{\ip}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\inter}[1]{\mathrm{int}{#1}}
\newcommand{\exter}[1]{\mathrm{ext}{#1}}
\newcommand{\cl}[1]{\mathrm{cl}{#1}}
\newcommand{\ds}{\displaystyle}
\newcommand{\vol}{\mathrm{vol}}
\newcommand{\cnt}{\mathrm{ct}}
\newcommand{\osc}{\mathrm{osc}}
\newcommand{\LL}{\mathbf{L}}
\newcommand{\UU}{\mathbf{U}}
\newcommand{\support}{\mathrm{support}}
\newcommand{\AND}{\;\wedge\;}
\newcommand{\OR}{\;\vee\;}
\newcommand{\Oset}{\varnothing}
\newcommand{\st}{\ni}
\newcommand{\wh}{\widehat}

% Some proof environment customization
\let\oldproof\proof
\renewcommand{\proof}{\color{blue}\oldproof}
\let\oldproofname=\proofname
\renewcommand*{\proofname}{Answer proposal}
\renewcommand{\qedsymbol}{$\blacksquare$}


%Pagination stuff.
\setlength{\topmargin}{-.3 in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textheight}{9.in}
\setlength{\textwidth}{6.5in}
\pagestyle{empty}



\begin{document}


\begin{center}
{\Large Reinforcement Learning \hspace{0.5cm} S\&B}\\
\textbf{Tabular Solution Methods}\\ %You should put your name here
\end{center}

\vspace{0.2 cm}


\subsection*{Exercises for chapter 2 : Multi-armed bandits}
\begin{enumerate}
    \item In $\epsilon$-greedy action selection, for the case of two actions and $\epsilon = 0.5$, what is
    the probability that the greedy action is selected?
    \begin{proof}
        $\epsilon$ defines the probability to take any action independantly of it's action-value estimate (including the greedy one). So, if we call $p$ the probability of taking the greedy action and $n$ the number of possible action to chose from, then $p = (1 - \epsilon) + \epsilon * \frac{1}{n}$ hence for $\epsilon = 0.5$ and $n = 2$, $p = 0.75$.
    \end{proof}

    \item Consider a k -armed bandit problem with k = 4 actions, denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using $\epsilon$-greedy action selection, sample-average action-value estimates, and initial estimates of $Q_1 (a) = 0$, for all a. Suppose the initial sequence of actions and rewards is $A_1 = 1, R_1 = -1, A_2 = 2, R_2 = 1, A_3 = 2, R_3 = -2, A_4 = 2, R_4 = 2, A_5 = 3, R_5 = 0$. On some of these time steps the "$\epsilon$ case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this possibly have occurred?
    \begin{proof}
        Let's answer the second question quickly, the $\epsilon$ case might have occured in any of the step. Now on the first question we simply have to compute the action-values for each step. Since they are updated one at a time, we only note the updated one at each time step knowing that $Q_1(a) = 0$ for all a. So we have :
        \[ Q_2(1) = -1, Q_3(2) = 1, Q_4(2) = -0.5, Q_5(2) = \frac{1}{3}, Q_6(3) = 0 \]
        Each time an action with non maximal action-value, namely at step 4 and 5, is selected then we must be in the $\epsilon$ case.
    \end{proof}

    \item In the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively.
    \begin{proof}
        Since $Q_t(a) \lra q_\star(a)$, in the long run, the estimate of the action-value should be the same in both case. However, the probability of selecting a suboptimal action is much higher (ten times) for $\epsilon = 0.1$ than for $\epsilon = 0.01$. Hence the probability of selecting the right action will be respectively $0.91$ and $0.991$ making it $1.089$ times more probable to select the best action in the second case. In terms of cumulative reward, let's say the best action has mean reward $r$ and the other action have mean reward $r'$ averaged on all other actions. Then at each time step, the reward for $\epsilon = 0.01$ would be $0.991r + 0.009r'$ while it would be $0.91r + 0.09r$ for $\epsilon = 0.1$.
    \end{proof}

    \item If the step-size parameters, $\alpha_n$, are not constant, then the estimate $Q_n$ is a weighted average of previously received rewards with a weighting different from that given by (2.6). What is the weighting on each prior reward for the general case, analogous to (2.6), in terms of the sequence of step-size parameters?
    \begin{proof}
        From the update rule we have :
        \begin{align*}
            Q_{n+1} = Q_n + \alpha_n(R_n - Q_n)\\
            Q_{n+1} = \alpha_nR_n + (1-\alpha_n)Q_n\\
            Q_{n+1} = \alpha_nR_n + (1-\alpha_n)(\alpha_{n-1}R_{n-1} + (1-\alpha_{n-1})Q_{n-1})\\
            Q_{n+1} = \alpha_nR_n + (1-\alpha_n)\alpha_{n-1}R_{n-1} + (1-\alpha_n)(1-\alpha_{n-1})Q_{n-1}\\
            \ldots\\
            Q_{n+1} = \sum_{i=1}^{n}\alpha_iR_i\prod_{j=i}^{n-1}(1 - \alpha_{j+1}) + \prod_{i=1}^{n}(1-\alpha_i)Q_1
        \end{align*}
    \end{proof}
    
    \item Design and conduct an experiment to demonstrate the
    difficulties that sample-average methods have for nonstationary problems. Use a modified version of the 10-armed testbed in which all the $q_{*}(a)$ start out equal and then take independent random walks (say by adding a normally distributed increment with mean 0 and standard deviation 0.01 to all the $q_*(a)$ on each step). Prepare plots like Figure 2.2 for an action-value method using sample averages, incrementally computed, and another action-value method using a constant step-size parameter, $\alpha = 0.1$. Use $\epsilon = 0.1$ and longer runs, say of 10,000 steps.
    \begin{proof}
        See code.
    \end{proof}
\end{enumerate}
\end{document}